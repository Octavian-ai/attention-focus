all_input_path: ./experiments/eeb7a45/input/all_input.tfrecords
attention_output_activation: abs
balance_batch: 20
batch_size: 32
eval_holdback: 0.1
eval_input_path: ./experiments/eeb7a45/input/eval_input.tfrecords
finder_initial_lr: 1.0e-06
input_dir: ./experiments/eeb7a45/input
kb_list_size: 2
kb_vector_length: 12
kb_vector_type: orthogonal
learning_rate: 4.0e-05
limit: null
log_level: INFO
max_gradient_norm: 8
max_steps: 500000
model_dir: ./experiments/eeb7a45/model/25630457
modes: [eval, train]
number_of_questions: 5000
output_dir: ./experiments/eeb7a45/output
predict_holdback: 0
train_input_path: ./experiments/eeb7a45/input/train_input.tfrecords
use_attention_focus: false
use_lr_decay: true
use_lr_finder: false
use_summary_scalar: false
warm_start_dir: null
