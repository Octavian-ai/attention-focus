all_input_path: ./experiments/3786c2b/input/all_input.tfrecords
attention_output_activation: tanh
balance_batch: 20
batch_size: 32
debug: false
eval_holdback: 0.1
eval_input_path: ./experiments/3786c2b/input/eval_input.tfrecords
finder_initial_lr: 1.0e-06
input_dir: ./experiments/3786c2b/input
kb_list_size: 2
kb_vector_length: 12
kb_vector_type: positive
learning_rate: 0.00023456
limit: null
log_level: INFO
max_gradient_norm: 8
max_steps: 300000
model_dir: ./experiments/3786c2b/model/25657621
modes: [eval, train]
number_of_questions: 5000
output_dir: ./experiments/3786c2b/output
predict_holdback: 0
train_input_path: ./experiments/3786c2b/input/train_input.tfrecords
use_attention_focus: false
use_lr_decay: true
use_lr_finder: false
use_summary_scalar: false
warm_start_dir: null
