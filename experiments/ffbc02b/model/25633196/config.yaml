all_input_path: ./experiments/ffbc02b/input/all_input.tfrecords
attention_output_activation: tanh
balance_batch: 20
batch_size: 32
eval_holdback: 0.1
eval_input_path: ./experiments/ffbc02b/input/eval_input.tfrecords
finder_initial_lr: 1.0e-06
input_dir: ./experiments/ffbc02b/input
kb_list_size: 2
kb_vector_length: 12
kb_vector_type: positive
learning_rate: 0.0006
limit: null
log_level: INFO
max_gradient_norm: 8
max_steps: 300000
model_dir: ./experiments/ffbc02b/model/25633196
modes: [eval, train]
number_of_questions: 5000
output_dir: ./experiments/ffbc02b/output
predict_holdback: 0
train_input_path: ./experiments/ffbc02b/input/train_input.tfrecords
use_attention_focus: true
use_lr_decay: true
use_lr_finder: false
use_summary_scalar: false
warm_start_dir: null
